{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10211,
          "databundleVersionId": 111096,
          "sourceType": "competition"
        },
        {
          "sourceId": 15520,
          "sourceType": "datasetVersion",
          "datasetId": 11167
        },
        {
          "sourceId": 38454,
          "sourceType": "datasetVersion",
          "datasetId": 2709
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/watermalone-pro/python_uw/blob/main/exercise5_random_forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap\n",
        "Here's the code you've written so far."
      ],
      "metadata": {
        "id": "kQEx7JzvRcGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code you have previously used to load data\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "\n",
        "# Path of the file to read\n",
        "iowa_file_path = '/content/train.csv'\n",
        "\n",
        "home_data = pd.read_csv(iowa_file_path)\n",
        "# Create target object and call it y\n",
        "y = home_data.SalePrice\n",
        "# Create X\n",
        "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
        "X = home_data[features]\n",
        "\n",
        "# Split into validation and training data\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
        "\n",
        "# Specify Model\n",
        "iowa_model = DecisionTreeRegressor(random_state=1)\n",
        "# Fit Model\n",
        "iowa_model.fit(train_X, train_y)\n",
        "\n",
        "# Make validation predictions and calculate mean absolute error\n",
        "val_predictions = iowa_model.predict(val_X)\n",
        "val_mae = mean_absolute_error(val_predictions, val_y)\n",
        "print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
        "\n",
        "# Using best value for max_leaf_nodes\n",
        "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
        "iowa_model.fit(train_X, train_y)\n",
        "val_predictions = iowa_model.predict(val_X)\n",
        "val_mae = mean_absolute_error(val_predictions, val_y)\n",
        "print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n"
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "aZDUqp6bRcGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ed5f8a-71ab-4f85-9916-8ba22d96f3e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE when not specifying max_leaf_nodes: 29,653\n",
            "Validation MAE for best value of max_leaf_nodes: 27,283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "Data science isn't always this easy. But replacing the decision tree with a Random Forest is going to be an easy win."
      ],
      "metadata": {
        "id": "TAZjyXgCRcG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Use a Random Forest"
      ],
      "metadata": {
        "id": "hSlB9tdjRcG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Define the model. Set random_state to 1\n",
        "rf_model = RandomForestRegressor(random_state=1)\n",
        "\n",
        "# fit your model\n",
        "rf_model.fit(train_X, train_y)\n",
        "\n",
        "\n",
        "# Calculate the mean absolute error of your Random Forest model on the validation data\n",
        "rf_val_mae = rf_model.predict(val_X)\n",
        "\n",
        "\n",
        "print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n",
        "\n"
      ],
      "metadata": {
        "id": "I_Qye_DURcG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8faeb5-9267-4b16-e2e2-150bf37562c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation MAE for Random Forest Model: [187439.65       149083.25       129767.58        85257.\n",
            " 149803.09       272586.79       326443.03       146521.5\n",
            " 220213.25       230411.5        173464.5        102247.36\n",
            " 204792.21       245101.54       255794.73       113153.5\n",
            " 113356.         146016.24       191653.77       142532.6\n",
            " 148765.5        194716.4        221809.         315077.49\n",
            " 105664.         169499.5        126757.58       191800.86\n",
            " 500298.02       128329.04       146786.36       100409.66666667\n",
            " 121062.5        107875.         132895.83       345668.25\n",
            " 114746.5        109237.71       289211.26       109256.5\n",
            " 145510.75       132667.5        105491.         115135.75\n",
            " 182216.         172469.          89118.5        196786.79\n",
            " 243525.29       221627.1        126305.5        393993.55\n",
            " 105281.         238323.83       191915.79       111486.5\n",
            " 128926.         174488.         124880.25       170135.03\n",
            " 166700.         317512.85       119112.         126428.\n",
            " 165634.         125155.5        133505.98       225090.7\n",
            " 156372.6        151839.64       224998.28       133699.7\n",
            " 318265.06       153992.84       130803.71       224836.3\n",
            " 169317.         112045.5        353945.28       195846.8\n",
            " 202634.59       134473.18       130205.1        153517.5\n",
            " 179145.         144430.36       153471.         164623.62\n",
            " 191630.7        171457.5        243688.66       156994.5\n",
            " 124509.5        146029.24       123507.71       110219.4\n",
            " 121706.08       138924.         135340.         144079.59\n",
            " 187627.18       127850.73333333 125000.03       117870.\n",
            " 109503.08       149267.44       192014.39       138555.74\n",
            " 153952.19       309964.42       115983.5        173658.93\n",
            " 144523.5        195289.59       229389.48       179807.77\n",
            " 240166.32       122655.5        173794.82       198784.02\n",
            " 155173.         253093.86       335603.22       139632.\n",
            " 227306.54       158736.37       316049.76       122802.\n",
            " 187598.44       214576.63       269662.07       124458.5\n",
            " 127715.25       107544.         100862.5        203085.\n",
            " 557822.1        297632.64       235094.         113951.16\n",
            " 150495.         290039.5        139388.5        199542.1\n",
            " 135379.46       227540.94       112555.         222135.3\n",
            " 218986.29       127137.24       180233.25       180880.6\n",
            " 126278.         180738.         173939.         338562.16\n",
            "  86860.         144701.24       103908.75       139674.1\n",
            " 104470.5        106159.84       152506.06       144896.2\n",
            " 141606.6        144332.         176287.         161693.34\n",
            " 148811.         124563.         243979.1        185385.24\n",
            " 235895.38       296466.53       189403.25       139364.\n",
            " 182035.5        210152.71       132663.96       148358.5\n",
            " 125608.08       170714.76       184350.18       126784.82\n",
            " 275975.44       175424.14       395770.23       286930.03\n",
            " 154651.11       118034.5        114312.         151984.42\n",
            "  91189.33333333 150178.08333333 126310.72       307206.53\n",
            " 257118.87       142856.5        148190.25        71703.5\n",
            " 187530.3        232009.63       166689.5        186362.\n",
            " 275872.74       109350.04       189618.19       325292.59\n",
            " 266024.58       244653.4        185677.2        110017.34\n",
            " 167486.73       134781.         273058.82       255291.95\n",
            " 122149.5         96260.         188690.5         86519.72\n",
            " 609470.96       120787.         170355.5        170172.7\n",
            " 106426.         115271.8        199903.41       183842.26\n",
            " 168989.63571429 173434.         124343.5        185548.\n",
            " 112474.5        127609.42       270973.06       133745.06\n",
            " 293087.67       116358.48       146489.32       293240.26\n",
            " 261006.63       148282.1        148333.         163170.32\n",
            " 138944.5        131167.         146302.5        165071.5\n",
            " 164632.09       160321.27       119811.58       115455.2\n",
            " 181051.54       157211.         151620.         113760.\n",
            " 178026.          81468.33       335180.88       131141.\n",
            " 193692.03       220201.         213366.97       184467.9\n",
            " 124498.04       182581.5        155761.75       135034.5\n",
            " 144176.          98432.         172969.5        178899.77\n",
            " 110214.54       132350.         175524.         132746.46\n",
            " 118704.38       136847.         199567.9        135113.\n",
            " 139779.54       135251.         132969.21       151249.\n",
            " 236750.33       234125.7        129019.18       112218.08\n",
            " 268633.17       113800.25       112169.5        270273.33\n",
            " 142687.         118706.66       193989.62       200493.\n",
            " 180013.35       241505.51       152234.         114292.25\n",
            " 151095.5        144472.5        134803.         178551.\n",
            " 143555.01333333 272760.         137916.         184546.5\n",
            " 165375.56       192219.87       154823.         168881.03\n",
            "  95577.         248745.84       147720.84       101184.5\n",
            " 259193.2        185586.         180433.89       174291.18\n",
            " 185896.95       183600.         125930.21       243360.18\n",
            " 134696.25       363444.75       163915.01       224671.21\n",
            " 152015.43       131714.34       136313.         215748.2\n",
            " 252742.84       272112.19       185320.         160618.64\n",
            " 139991.2        140396.5        277316.9        181259.39\n",
            " 247413.5        196697.          88518.5        236797.62\n",
            " 115513.25       159140.96       139489.         151250.72\n",
            " 263169.49       101315.         190183.33       147561.\n",
            " 369019.05       128854.5        146443.1        149975.64\n",
            " 200523.56       153335.         117320.5        299389.65\n",
            " 172783.65333333 180620.89       132581.04       287663.41\n",
            " 207253.16      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, you have followed specific instructions at each step of your project. This helped learn key ideas and build your first model, but now you know enough to try things on your own.\n",
        "\n",
        "Machine Learning competitions are a great way to try your own ideas and learn more as you independently navigate a machine learning project.\n",
        "\n",
        "# Keep Going\n",
        "\n",
        "You are ready for Machine Learning Competitions\n"
      ],
      "metadata": {
        "id": "56Ikp_zURcG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tb85Bh1HRcG5"
      }
    }
  ]
}